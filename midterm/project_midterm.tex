\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{url}
\usepackage{graphicx}

\title{Mid-Term Paper : \\Freesound General-Purpose Audio Tagging Challenge}


\author{
Sarah Gross \\
Master of Advanced Computing \thanks{\url{http://ac.cs.tsinghua.edu.cn/}}\\
Tsinghua University\\
\texttt{leihy17@mails.tsinghua.edu.cn} \\
\And
Usama Zafar\\
Master of Advanced Computing\\
Tsinghua University\\
\texttt{zafaru10@mails.tsinghua.edu.cn} \\
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
	Sound tagging has been studied for at least a couple of decades now. Among the different types of sounds the most prevalent research areas have been music, speech and enviromental sounds. Some sounds are distinct and instantly recognizable, like a baby's laugh or the strum of a guitar. Other sounds aren't clear and are difficult to pinpoint, or are drowned in a mix of sounds that are difficult to identify individually.\\
	\newline
    Partly because of the vastness of sounds we experience, no reliable automatic general-purpose audio tagging systems exist. Currently, a lot of manual effort is required for tasks like annotating sound collections and providing captions for non-speech events in audiovisual content.\\
	\newline
    This project's goal is to be able to recognize an increased number of sound events of very diverse nature, and to leverage subsets of training data featuring annotations of varying reliability.
\end{abstract}

\section*{Introduction}
    Our life is surrounded by various sounds: speech, music, animal call, aircraft, traffic, even the sound of typing words, clicking the mouse, etc. Sounds can be roughly grouped into three clusters: human voice, artificial sound, and non-artificial/natural sound.\\
    \newline
    Human voice refers to sounds created by people physically such as speech, cough, and singing. Artificial sounds refer to sounds created by human activities such as traffic, aircraft, and music. Non- artificial sounds include sounds created by nature such as wind, rain, land animal, insects and marine life.\\
    \newline
    These sounds make the world exclamatory and colourful. All these sounds carry information and have their own characteristics. In order to categorise different kinds of sounds and study them separately, tagging is introduced into the area of sound analysis. The act of tagging, in this context refers to the action of adding text based on metadata and annotations to specific non-textual information and data.\\
    \newline
    Initially, people classified and documented all information manually. If you close your eyes, could you tell the difference between the sound of a chainsaw and a blender? Probably not. With the development of machine technology, especially the computer science, people started to study new ways of automatic tagging, not only due to its accuracy but also due to its performance. A lot of classification work has been solved efficiently for music, speech and environmental sounds.\\
    \newline
    However, despite the good performance, these automatic tagging machines still need information from the metadata of targets. The metadata is collected manually in several ways. Besides one model suited for tagging one classification of sounds might not be suited for the other classes. And often times than not it is the case that sounds are not distinguished into their respective classes. This is where machine learning comes into play, in cases such as these, there is a need for general purpose tagging systems.


\section{Dataset}
	\subsection{About the dataset}
		Freesound Dataset Kaggle 2018 (or FSDKaggle2018 for short) is an audio dataset containing 18,873 .wav files annotated with 41 labels from Google's AudioSet Ontology \cite{cite1}:\\

			"Acoustic\_guitar", "Applause", "Bark", "Bass\_drum", "Burping\_or\_eructation", "Bus", "Cello", "Chime", "Clarinet", "Computer\_keyboard", "Cough", "Cowbell", "Double\_bass", "Drawer\_open\_or\_close", "Electric\_piano", "Fart", "Finger\_snapping", "Fireworks", "Flute", "Glockenspiel", "Gong", "Gunshot\_or\_gunfire", "Harmonica", "Hi-hat", "Keys\_jangling", "Knock", "Laughter", "Meow", "Microwave\_oven", "Oboe", "Saxophone", "Scissors", "Shatter", "Snare\_drum", "Squeak", "Tambourine", "Tearing", "Telephone", "Trumpet", "Violin\_or\_fiddle", "Writing"\\
			\newline

		More information is accessible on 
		\begin{center} 
		\url{https://www.kaggle.com/c/freesound-audio-tagging/data}
		\end{center}

		We use a first subset au audio files \emph{audio\_train} to train our model, and a second subset \emph{audio\_test} for evaluating our model.\\
		The labels for train dataset are wirtten in train.csv. It contains the following information:
		\begin{itemize}
		    \item fname: the file name
		    \item label: the audio classification label (ground truth
		    \item manually\_verified: Boolean (1 or 0) flag to indicate whether or not that annotation has been manually verified.
		\end{itemize}

	\subsection{Dataset analysis}
		We first plotted the distribution of samples per label \ref{fig:category_distribution}.
		\begin{figure}
		  \includegraphics[width=\linewidth]{category_distribution.png}
		  \caption{Distribution of samples per category}
		  \label{fig:category_distribution}
		\end{figure}
		We notice two phenomena :
		\begin{itemize}
		    \item There is a highly unequal distribution of samples over the labels
		    \item The number of verified data varies over the labels
		\end{itemize}

		We can visualize the difference between the samples by using some useful tools from the python package for music and audio analysis LibROSA\footnote{\url{http://librosa.github.io/librosa/}}. We picked randomly 4 samples \emph{Oboe, Fireworks, Cello, Harmonica} and plot various graphs \ref{graphs} :

		\begin{itemize}
		    \item The Waveplot
		    \item The Spectrogram
		    \item The Log Spectrogram
		\end{itemize}


		\begin{figure}
		  \includegraphics[width=\linewidth]{waveplot.png}
		  \includegraphics[width=\linewidth]{spectrogram.png}
		  \includegraphics[width=\linewidth]{logspectrogram.png}
		  \caption{Useful tools for analysis sounds provided by Librosa}
		  \label{fig:graphs}
		\end{figure}

		We see that we get very different kinds of outputs and there is no way to visually guess which graphs belong to which sound.
		Usually, in sound analysis, the raw samples are very complex and it is difficult to get valuable information directly from them, especially if they contain noise.
		This is why we won't use the raw samples but extract some features from the sample which will, in contrary, give significant information that will help us make the classification.


\section{Feature Extraction}

	\subsection{Theory}
		Why use feature extraction ? Use mathematics \url{http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/}
		Mel-frequency Cepstrum  is a convenient spectral representation of a sound because itd scale is based on the human hearing perception.

	\subsection{Implementation}
		We used the librabry LibROSA, which appears in many works related to sound classification such as K. Pizcack's \cite{cite2}.\\

		Explain why not only use mfcc ?
		\newline
		Here are some examples of the output of MFCC using Librosa :
		\begin{figure}
		  \includegraphics[width=\linewidth]{mfcc.png}
		  \caption{Mel Frequency Cepstral Coefficients}
		\end{figure}
	
\section{A Simple Neural Network}
		In our project proposal, we first proposed using Hidden Markov Model algorithm to classifiy the sounds.\\
		This choice was made with very little knowledge of this technique, because a few papers using it obtained good results \cite{cite3} \cite{cite4}. However after extensive research of the inner functionning of HMMs, we understood why many other papers of our survey disapproved using this algorithm \cite{cite5} \cite{cite6}. HMM's idea is to start from a known final state, and try to guess which sequence of actions was followed in order to get to that result. This path, or sequence of actions, is unknown hence the adjective \emph{hidden}. HMM are therefore very useful in cases there is an actual path to find, for instance in board games or speech recognition : in the latter case, the path consists of \emph{each word} spoken in the sentence.\\
		\newline
		However, in our case, we don't want to extract and analyse every little part that consists our general sound input, we only want to classify it. Therefore HMM is not appropriate for making this kind of task on our data.\\
		\newline
		After some reflection, a good range of algorithms being able to classifiy such complex data appear to be \textbf{Neural Networks}. Few papers in our proposal, like Zhang et. al.\cite{cite6}, mentionned this technique as it is very novel and most papers surveyed dated back to before 2014. However we could find many recent works on the net confirming this technique was effective for environment sound classification.\\
		We first tried to make a simple Neural Network and test it on our extracted feature data.\\

		We used the Tensorflow\footnote{\url{http://www.tensorflow.org}} machine learning framework to create the Neural Network and ran it on a NV6 Microsoft Azure Data Science Virtual Machine.

\section{Future Work}
	We will try to incrementally complexify the structure of out Neural Network. We will first use a Convolutional Neural Network, as presented in Zhang et. al.\'s work\cite{cite6}. Since we are processing an audio signal, a 1-dimension CNN should be enough, but we can try using 2D CNN if the results are not concluant. We will then try to combine different CNNs to get more precise results if we have time.

\subsubsection*{Acknowledgments}

	This project was led during the Machine Learning course from the Master of Advanced Computing of Tsinghua University. The Azure Virtual Machines were kindly provided by Pr. Jun Zhu\url{http://ml.cs.tsinghua.edu.cn/~jun/index.shtml} of the Machine Learning department of Tsinghua University.

\nocite{*}
\bibliographystyle{unsrt}
\bibliography{project_midterm}

\end{document}
