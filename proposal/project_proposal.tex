\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{url}


\title{Project Proposal : \\Freesound General-Purpose Audio Tagging Challenge}


\author{
Sarah Gross \\
Master of Advanced Computing \thanks{\url{http://ac.cs.tsinghua.edu.cn/}}\\
Tsinghua University\\
\texttt{leihy17@mails.tsinghua.edu.cn} \\
\And
Usama Zafar\\
Master of Advanced Computing\\
Tsinghua University\\
\texttt{zafaru10@mails.tsinghua.edu.cn} \\
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
	Sound tagging has been studied for at least a couple of decades now. Among the different types of sounds the most prevalent research areas have been music, speech and enviromental sounds. Some sounds are distinct and instantly recognizable, like a baby's laugh or the strum of a guitar. Other sounds aren't clear and are difficult to pinpoint, or are drowned in a mix of sounds that are difficult to identify individually.\\
	\newline
    Partly because of the vastness of sounds we experience, no reliable automatic general-purpose audio tagging systems exist. Currently, a lot of manual effort is required for tasks like annotating sound collections and providing captions for non-speech events in audiovisual content.\\
	\newline
    This project's goal is to be able to recognize an increased number of sound events of very diverse nature, and to leverage subsets of training data featuring annotations of varying reliability.
\end{abstract}

\section*{Introduction}
    Our life is surrounded by various sounds: speech, music, animal call, aircraft, traffic, even the sound of typing words, clicking the mouse, etc. Sounds can be roughly grouped into three clusters: human voice, artificial sound, and non-artificial/natural sound.\\
    \newline
    Human voice refers to sounds created by people physically such as speech, cough, and singing. Artificial sounds refer to sounds created by human activities such as traffic, aircraft, and music. Non- artificial sounds include sounds created by nature such as wind, rain, land animal, insects and marine life.\\
    \newline
    These sounds make the world exclamatory and colourful. All these sounds carry information and have their own characteristics. In order to categorise different kinds of sounds and study them separately, tagging is introduced into the area of sound analysis. The act of tagging, in this context refers to the action of adding text based on metadata and annotations to specific non-textual information and data.\\
    \newline
    Initially, people classified and documented all information manually. If you close your eyes, could you tell the difference between the sound of a chainsaw and a blender? Probably not. With the development of machine technology, especially the computer science, people started to study new ways of automatic tagging, not only due to its accuracy but also due to its performance. A lot of classification work has been solved efficiently for music, speech and environmental sounds.\\
    \newline
    However, despite the good performance, these automatic tagging machines still need information from the metadata of targets. The metadata is collected manually in several ways. Besides one model suited for tagging one classification of sounds might not be suited for the other classes. And often times than not it is the case that sounds are not distinguished into their respective classes. This is where machine learning comes into play, in cases such as these, there is a need for general purpose tagging systems.

\section{Kaggle Project}

	We chose the project \textbf{Freesound General-Purpose Audio Tagging Challenge}, as we were both interested on working on sound samples.
	\begin{center}
	   \url{https://www.kaggle.com/c/freesound-audio-tagging}
	\end{center}
	The project bears the following deadlines:
	\begin{itemize}
	    \item July 24, 2018 - Entry deadline.
	    \item July 24, 2018 - Team Merger deadline, the last day participants may join or merge teams.
	    \item July 31, 2018 - Final submission deadline.
	\end{itemize}
    

\section{Related Work}
    Earlier work in the domain of audio signal processing has focused on three broad categories music, speech and environmental sound tagging individually, there has almost been very little to no work in the direction of a unified general purpose sound tagger that would work across these spectrums.
    The main difficulty, is our case, is dealing with sounds with very large bandwith, no structure --- in opposite to human speech ---, bearing possible noise. These differences make the usual sound classification techniques, such as Hidden Markov Models (HMM) or Gaussian Mixture Models (GMM), be much less accurate with this kind of data.

    While Cowling et. al. \cite{cite1} and Zhang et. al. \cite{cite2} reject alltogether the idea of using HMM and concentrale on less conventional classification algorithms, Dufaux et. al. \cite{cite3} present good results robust to the presence of noise with both HMM and GMM, with around 98\% success with little noise and 57\% in average for maximum noise. However, they needed to aggregate models for each noise level in order to acheive an algorithm working in any noise environement, making the computation time much longer. On the other hand, Casey \cite{cite4} focused on using decorrelated spectral features coupled with HMM for computing similarity and generating source classifications. His work has been incorporated into the MPEG-7 international standard. 

    As for GMM, Cowling et. al. \cite{cite1} show the huge gap between the performance for music instruments and general non-speech noice, with 90\% for the first, but at most 46\% for the latter.
    
    Duan et. al. \cite{cite5} perfomed a survey of tagging techniques used/proposed across all three spectrums in 2012. Chachada et. al. \cite{cite6} performed a survey of enviromental sound recognition techniques. Again Chu et. al. \cite{cite7} have made the selection of appropriate features for environmental sound recognition focus of their paper and furthermore presented \cite{cite8} a system for environmental sound tagging with time-frequency audio features.

    Since environmental sound is unstructured and has a kind of random composition, Zhang et. al. \cite{cite2} propose to take inspiration from other well-known unstructured data --- images --- to build efficient classification. With Convolutional Neural Nets (CNN), they get highly reliable results with 93\% mean accuracy, its strong point being correct despite the noise in the audio samples.

    Another important point to consider in our research is first extracting the some typical features in the signal before performing any classification. This process is quickly evoked by Dufaux et. al. \cite{cite3}, and more detailled by Cowling et. al. \cite{cite1}. They split the feature extraction techniques in two categories : stationary, meaning only based on frequence, and non-stationary, meaning based on both time and frequence. They present the effectiveness of 7 extraction techniques (4 stationary, 3 non-stationary) coupled with 5 different classification techniques : Learning vector quantization, Artificial neural networks, Dynamic time warping, Long-term statistics, Gaussian mixture models. They show that the final performances of classification vary with the extraction technique. Therefore we must consider carefully how we will extract the features in our algorithm.

\section{Our method}
	It appears that papers that reject HMM actually didn't test it and just speculate on its inefficiency. On the other hand, research which did implement them \cite{cite3} \cite{cite4} present excellent results, with a list of data very close to ours. This is why we will proceed as follows :
	\begin{enumerate}
		\item We implement a Hidden Markov Model to classify the sounds, using a specific Python Library\footnote{PythonHMM : \url{https://github.com/jason2506/PythonHMM}}.
		\item If the model is not efficient enough, we increase the complexity by adding Gaussian Mixture Models.
		\item If HMM and GMM give unsastifying results, we will change method and use Convolutional Neural Networks, as described in \cite{cite2}.
	\end{enumerate}
	We will use Convolutional Neural Networks as a last resort due to its complexity. We wanted first to try simpler models.\\
	\newline
	An important part of our work will be to format the data to input in the model, as well as extracting the significant features.

\section{Data}
	Sounds in the dataset include things like musical instruments, human sounds, domestic sounds, and animals from Freesound’s library, annotated using a vocabulary of more than 40 labels from Google’s AudioSet ontology. 
	More information is accessible on 
	\begin{center} 
	\url{https://www.kaggle.com/c/freesound-audio-tagging/data}
	\end{center}

	\subsection{Data fields}
		Each row of the train.csv file contains the following information:
			\begin{itemize}
			    \item fname: the file name
			    \item label: the audio classification label (ground truth
			    \item manually\_verified: Boolean (1 or 0) flag to indicate whether or not that annotation has been manually verified; see below for additional background
			\end{itemize}

	\subsection{About this dataset}
		Freesound Dataset Kaggle 2018 (or FSDKaggle2018 for short) is an audio dataset containing 18,873 audio files annotated with labels from Google's AudioSet Ontology \cite{cite9}.\\
		\newline
		All audio samples in this dataset are gathered from Freesound \cite{cite10} (CC license) and were provided as uncompressed PCM 16 bit, 44.1 kHz, mono audio files.\\
		\newline
		The Data was labelled by first carrying an automatic annotation using the AudioSet Ontology Labels. Then, a part of this data was manually validated by human participants, which we will call \emph{ground truth data}. Some of the annotations have not been manually validated and therefore some of them could be inaccurate. Nonetheless, it was estimated that at least 65-70\% of the non-verified annotations per category in the train set are indeed correct. More details about the data labeling process can be found in \cite{cite11}.
		\newline
		FSDKaggle2018 sounds are unequally distributed in the following 41 categories of the AudioSet Ontology:\\

		"Acoustic\_guitar", "Applause", "Bark", "Bass\_drum", "Burping\_or\_eructation", "Bus", "Cello", "Chime", "Clarinet", "Computer\_keyboard", "Cough", "Cowbell", "Double\_bass", "Drawer\_open\_or\_close", "Electric\_piano", "Fart", "Finger\_snapping", "Fireworks", "Flute", "Glockenspiel", "Gong", "Gunshot\_or\_gunfire", "Harmonica", "Hi-hat", "Keys\_jangling", "Knock", "Laughter", "Meow", "Microwave\_oven", "Oboe", "Saxophone", "Scissors", "Shatter", "Snare\_drum", "Squeak", "Tambourine", "Tearing", "Telephone", "Trumpet", "Violin\_or\_fiddle", "Writing"\\
		\newline
		Here are some other relevant characteristics of FSDKaggle2018:
		\begin{itemize}
		    \item The dataset is split into a train set and a test set.
		    \item The train set is meant to be for system development and includes ~9.5k samples unequally distributed among 41 categories. The minimum number of audio samples per category in the train set is 94, and the maximum 300. The duration of the audio samples ranges from 300ms to 30s due to the diversity of the sound categories and the preferences of Freesound users when recording sounds.
		    \item Out of the ~9.5k samples from the train set, ~3.7k have manually-verified ground truth annotations and ~5.8k have non-verified annotations. The non-verified annotations of the train set have a quality estimate of at least 65-70\% in each category.
		    \item Non-verified annotations in the train set are properly flagged in train.csv so that participants can opt to use this information during the development of their systems.
		    \item The test set is composed of ~1.6k samples with manually-verified annotations and with a similar category distribution than that of the train set. The test set is complemented with ~7.8k padding sounds which are not used for scoring the systems.
		    \item All audio samples in this dataset have a single label (i.e. are only annotated with one label).
		\end{itemize}

\subsubsection*{Acknowledgments}

	This project was supported by Tsinghua University, in the wake of the course of Machine Learning from the Master of Advanced Computing.

\nocite{*}
\bibliographystyle{unsrt}
\bibliography{project_proposal}

\end{document}
